---
title: "Cross Validation"
author: "Christian Pascual"
date: "4/2/2019"
output: html_document
---

```{r setup, message = FALSE}
library(glmnet) # LASSO
library(mgcv) # GAMs
library(modelr) # cross-validation help

# Loading in the dataset
admit.data = read.csv(file = "./admission_predict.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(
    gre.std = (gre_score - mean(gre_score)) / sd(gre_score),
    toefl.std = (toefl_score - mean(toefl_score)) / sd(toefl_score),
    cgpa.std = (cgpa - mean(cgpa)) / sd(cgpa),
    uni.rating = university_rating,
    sop.strength = sop,
    lor.strength = lor
  ) %>% 
  dplyr::select(serial_no, gre.std:lor.strength, research, chance_of_admit)
```

```{r train-test-split }
set.seed(8106)
# Set up indices for splitting between training and test datasets
folds = crossv_kfold(admit.data, k = 10)
```

### Linear CV
Cross validation was performed in two ways: 1)using map function and 2) using loops. They had the same results.. YAY! we should prob use the loop method to stay consistent?
MSE: 0.003688345

```{r}
# linear model 1

# fit models on train data
lm.models = map(folds$train, ~ lm(chance_of_admit ~ gre.std + toefl.std + cgpa.std 
             + uni.rating + sop.strength + lor.strength 
             + research, data = .))

# test predictions
lm.mse = map2_dbl(lm.models, folds$test, modelr::mse)

mean(lm.mse)
```

```{r}
#linear models 2
for (k in 1:nrow(folds)) {
  
  # prepare test and train data
  train.idx = folds[k,1][[1]][[toString(k)]]$idx
  train = admit.data[train.idx,]
  test = admit.data[-train.idx,]
  
  y.test = test$chance_of_admit
  
  # fit models on train
  lin.models = lm(chance_of_admit ~ gre.std + toefl.std + cgpa.std 
             + uni.rating + sop.strength + lor.strength 
             + research, data = train)
  
  # test 
  lin.pred = predict(lin.models, newdata = test)
  
  lin.mse[k] = mean((lin.pred - y.test)^2)

}

lin.mse.final = mean(lin.mse)
```

### Lasso CV
MSE:0.003684451
```{r}
# lasso
for (k in 1:nrow(folds)) {
  
  # prepare test and train data
  train.idx = folds[k,1][[1]][[toString(k)]]$idx
  train = admit.data[train.idx,]
  test = admit.data[-train.idx,]
  
  # train matrix
  X.train = train %>% dplyr::select(gre.std:research)
  fmt.X.train = model.matrix(~ gre.std + toefl.std + cgpa.std + uni.rating + 
                   sop.strength + lor.strength + research, X.train)
  y.train = train$chance_of_admit
  
  # test matrix
  X.test = test %>% dplyr::select(gre.std:research)
  fmt.X.test = model.matrix(~ gre.std + toefl.std + cgpa.std + uni.rating + 
                   sop.strength + lor.strength + research, X.test)
  y.test = test$chance_of_admit
  
  # fit models on train
  lasso.cv = cv.glmnet(fmt.X.train[,-1], y.train, alpha = 1, type.measure = "mse")
  lasso.models = glmnet(fmt.X.train[,-1], y.train, alpha = 1, 
                   lambda = lasso.cv$lambda.min)
  
  # test 
  lasso.pred = predict(lasso.models, s = lasso.cv$lambda.min, newx = fmt.X.test[,-1])
  
  lasso.mse[k] = mean((lasso.pred - y.test)^2)
  
}

lasso.mse.final = mean(lasso.mse)

```

### GAM CV
MSE: 0.003695414
```{r}
# gam

for (k in 1:nrow(folds)) {
  
  # prepare test and train data
  train.idx = folds[k,1][[1]][[toString(k)]]$idx
  train = admit.data[train.idx,]
  test = admit.data[-train.idx,]
  
  y.test = test$chance_of_admit
  
  # fit models on train
  gam.models =  gam(chance_of_admit ~ gre.std + s(toefl.std) + s(cgpa.std) 
               + uni.rating + sop.strength + lor.strength 
               + research, data = train)
  
  # test 
  gam.pred = predict(gam.models, newdata = test)
  
  gam.mse[k] = mean((gam.pred - y.test)^2)

}
  
gam.mse.final = mean(gam.mse)

```


### Results:
linear mse = 0.003688345
lasso mse = 0.003684451
gam mse = 0.003695414

The lasso model had the lowest resulting MSE from 10-fold cross-validation. However, all MSEs from linear, lasso, and gam models were very similar.



