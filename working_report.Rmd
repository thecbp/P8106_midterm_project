---
title: "Data Science II Midterm Report"
author: "Dayoung Yu (dry2115), Justin Hsie (jih2119), Christian Pascual (cbp2128)"
date: "3/18/2019"
output: 
  pdf_document:
    number_sections: true
    fig_width: 7
    fig_height: 6
    fig_caption: true
---

# Introduction

Applying to graduate programs is a harrowing process for any student. Between the statement of purpose, letters of recommendation, GRE and GPA, there's a lot of factors that can influence the acceptance or rejection of a hopeful student. Given data on the application components, predicting a student's chance of poses an interesting regression problem. From a student perspective, the ability to predict chances of admission would allow them to ground their expectations and plan for the future. 

The *Graduate Admissions* dataset from Kaggle contains data on about 500 Indian students applying for Master's programs in the United States. The response variable we hope to predict is `Chance of Admit`. The potential predictors are various Master's application components converted into continuous or categorical form, including: GRE score, TOEFL score, university rating, statement of purpose strength, letter of recommendations strength, cumulative GPA and the presence of research experience. 

## Data Cleaning 

```{r cleaning, message = FALSE, warning = FALSE }
library(tidyverse)
library(glmnet) # LASSO
library(mgcv) # GAMs
library(modelr) # cross-validation help
library(earth)
library(caret) # cross-validation help
library(kableExtra)
library(corrplot)

# Loading in the dataset
admit.data = read.csv(file = "./admission_predict.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::mutate(
    gre.std = (gre_score - mean(gre_score)) / sd(gre_score),
    toefl.std = (toefl_score - mean(toefl_score)) / sd(toefl_score),
    cgpa.std = (cgpa - mean(cgpa)) / sd(cgpa),
    uni.rating = university_rating,
    sop.strength = sop,
    lor.strength = lor
  ) %>% 
  dplyr::select(gre.std:lor.strength, research, chance_of_admit)
```

```{r echo = FALSE }
knitr::kable(head(admit.data), booktabs = T)
```

In its raw form, the dataset only needs minimal formatting before we can start modeling. GRE, CGPA and TOEFL are already continuous and don't need coercing, but they will be centered and scaled. For university rating, statement of purpose, and letter of recommendation strength, these variables are seemingly categorical, but they will be treated as continuous for easier interpretation and to reduce the number of dummy predictors. The presence of research was recoded as a proper binary variable. 

We will attempt to predict `Chance of Admit` using a total of 5 models: 3 linear models, ordinary linear regression, ridge regression and LASSO, and 2 non-linear models, a generalized additive and MARS model. We will be using the implementations in `lm`, `glmnet`, `gam` and `earth` to do the modeling. Before we start this process, we will explore how each of our predictors are interrelated and related to the response. 

This final product of seeks to create these models, compare them, and hopefully recommend one for use by future college hopefuls.  

# Exploratory Data Analysis

```{r, echo = FALSE}
cor.matrix = cbind(as.tibble(names(admit.data)), as.tibble(cor(admit.data)))

cor.matrix %>%
  kable("latex", caption = "Correlation matrix of predictors", digits = 4) %>%
  kable_styling(latex_options = c("hold_position","scale_down"))

x = model.matrix(chance_of_admit ~ ., admit.data)[,-1]
corrplot(cor(x))
```


From the correlation matrix, we see that all predictors except `research` are highly correlated with each other ($\rho$>0.5). The highest correlation occurs between `gre.std`, `toefl.std`, and `cgpa.std`, indicating that applicants who have high GPAs are more likely to have high test scores, and applicants who score well in one standardized test will more likely score well in the other. Including all three `gre.std`, `toefl.std`, and `cgpa.std` predictors may result in multicollinearity and highly variable model predictability. 


```{r, echo = FALSE}
x2 = model.matrix(chance_of_admit ~ gre.std+toefl.std+cgpa.std+sop.strength+lor.strength+uni.rating,
                  admit.data)[,-1]
y = admit.data$chance_of_admit
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
featurePlot(x2, y, plot = "scatter", labels = c("","chance_of_admit"),
            type = c("p"), layout = c(3, 2))
```

The exploratory plots indicate the existence of a positive relationship between all predictors and `Chance of Admit`. From the second row, we see that as GPA, GRE, and TOEFL scores increase, the chance of admittance increases. In addition, higher ranks on a 1:5 scale of statement of purpose strength, letter of recommendation strength, and university rating are associated with high chances of admittance. The plot for `research` was not included due to its binomial nature. However, the data indicate that presence of research experience is also positively associated with chance of admittance.


# Models

Our dataset only contains 7 predictors, so we will incorporate all of them into each of our 4 models. Each of these factors are requested in most Master's program applications, so we will assume all will have an impact on predicting the chance of admission.

## Linear Models

We plan to use ordinary linear regression as a *baseline* model to compare the other 4 models with. Although we suspect that all the variables will have an appreciable impact on the chance of admission, we still want to allow for the possibility that some predictors do not truly contribute. Furthermore, we also found high correlation between many of our predictors in our exploratory data analysis, so we also want to adjust for potential inflation of our regression coefficients. Therefore, we plan to use the ridge and LASSO models to shrink the coefficients and perform variable selection to adjust for these findings and hopefully improve predictions.  

## Non-linear Models

In our exploratory data analysis, we also found that some of our predictors (CGPA and TOEFL) had a slight nonlinear relationship with the chance of admission. We hope to use a GAM model and a MARS model to better capture these nonlinear relationships and produce improved predictions as a result. 

## Model Tuning

We used `cv.glmnet` to find the optimal $\lambda$ for both the ridge and LASSO models via 5-fold cross-validation. For the GAM, we found the optimal smoothing parameters via generalized cross-validation. Similarly, generalized cross-valiation was used to select the optimal number of knots in the MARS model.

## Findings

For each of the models, we used 10-fold cross-validation to evaluate the average test fold MSE as a measurement of predictive ability. Table # below shows a comparison of the average test fold MSE for each of the 5 models. 

```{r predict-cv, echo = FALSE }
set.seed(8106)
folds = crossv_kfold(admit.data, k = 10)
lin.mse = vector(length = 5)
lasso.mse = vector(length = 5)
ridge.mse = vector(length = 5)
gam.mse = vector(length = 5)
mars.mse = vector(length = 5)

tuning_grid = expand.grid(
  degree = 1:3, 
  nprune = seq(2, 50, length.out = 10) %>% floor()
  )

for (k in 1:nrow(folds)) {
  
  # prepare test and train data
  train.idx = folds[k,1][[1]][[toString(k)]]$idx
  train = admit.data[train.idx,]
  test = admit.data[-train.idx,]
  
  # train matrix
  X.train = train %>% dplyr::select(gre.std:research) %>% as.matrix(.)
  y.train = train$chance_of_admit
  
  # test matrix
  X.test = test %>% dplyr::select(gre.std:research) %>% as.matrix(.)
  y.test = test$chance_of_admit
  
  # fit models on train
  lin.model = lm(chance_of_admit ~ ., data = train)
  lasso.cv = cv.glmnet(X.train, y.train, alpha = 1, type.measure = "mse")
  lasso.model = glmnet(X.train, y.train, alpha = 1, lambda = lasso.cv$lambda.min)
  
  # fit models on train
  ridge.cv = cv.glmnet(X.train, y.train, alpha = 0, type.measure = "mse")
  ridge.model = glmnet(X.train, y.train, alpha = 0, lambda = ridge.cv$lambda.min)
  
  gam.model =  gam(chance_of_admit ~ gre.std + s(toefl.std) + s(cgpa.std) 
               + uni.rating + sop.strength + lor.strength 
               + research, data = train)
  mars.model = train(
    x = subset(train, select = -chance_of_admit),
    y = train$chance_of_admit,
    method = "earth",
    trControl = trainControl(method = "cv", number = 10),  
    tuneGrid = tuning_grid
    )
  
  lin.pred = predict(lin.model, newdata = test)
  lasso.pred = predict(lasso.model, s = lasso.cv$lambda.min, newx = X.test)
  ridge.pred = predict(ridge.model, s = ridge.cv$lambda.min, newx = X.test)
  gam.pred = predict(gam.model, newdata = test)
  mars.pred = predict(mars.model, newdata = test)
  
  lin.mse[k] = mean((lin.pred - y.test)^2)
  lasso.mse[k] = mean((lasso.pred - y.test)^2)
  ridge.mse[k] = mean((ridge.pred - y.test)^2)
  gam.mse[k] = mean((gam.pred - y.test)^2)
  mars.mse[k]  = mean((mars.pred - y.test)^2)
}
tib = tibble(
  `Model` = c("Linear", "LASSO", "Ridge", "GAM", "MARS"),
  `Avg. Test MSE` = c(mean(lin.mse),
                      mean(lasso.mse),
                      mean(ridge.mse),
                      mean(gam.mse),
                      mean(mars.mse))
) %>% arrange(`Avg. Test MSE`)
knitr::kable(tib, booktabs = T,
             caption = "Average Test Fold MSE between the 5 candidate models")
```

The LASSO model performed the best out of the 5 models, although all of them are comparable since they have similar average test fold MSEs. The difference between the best and worst models is on the $1^{-5}$ scale, which would have a negligible difference in terms of predicting a student's chance of admission. 



```{r estimate table, echo = FALSE}
# create matrix
X = admit.data %>% select(gre.std:research)
fmt.X = model.matrix(~ gre.std + toefl.std + cgpa.std + uni.rating + 
                   sop.strength + lor.strength + research, X)
y = admit.data$chance_of_admit

#linear model
glm.fit = glm(chance_of_admit ~ gre.std + toefl.std + cgpa.std + uni.rating + 
                   sop.strength + lor.strength + research, data = admit.data)

# lasso model
lasso.cv = cv.glmnet(fmt.X[,-1], y, alpha = 1, type.measure = "mse")
lasso.fit = glmnet(fmt.X[,-1], y, alpha = 1, lambda = lasso.cv$lambda.min)

# ridge model
ridge.cv = cv.glmnet(fmt.X[,-1], y, alpha = 0, type.measure = "mse",
                       lambda = exp(seq(-1, 10, length = 100)))
ridge.fit = glmnet(fmt.X[,-1], y, alpha = 0, lambda = ridge.cv$lambda.min)


# make table
Predictor = row.names(as.tibble(coef(glm.fit))) 

glm.coef = as.tibble(coef(glm.fit)) %>%
  rownames_to_column() %>%
  select("Predictor" = rowname, "Linear" = value)

lasso.coef = as.tibble(coef(lasso.fit)[,1]) %>%
  rownames_to_column() %>%
  select("Predictor" = rowname, "LASSO" = value)

ridge.coef = as.tibble(coef(ridge.fit)[,1]) %>%
  rownames_to_column() %>%
  select("Predictor" = rowname, "Ridge" = value) 

glm_lasso = full_join(glm.coef, lasso.coef, by = "Predictor")

coef.table = full_join(glm_lasso, ridge.coef, by = "Predictor") %>%
  column_to_rownames(var = "Predictor")

# print table
options(knitr.kable.NA = '*')

coef.table %>%
kable("latex", booktabs = T, linesep = "", escape = F, digits = 4,
      caption = "Coefficient estimates of the three linear models") %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Coefficient Estimates" = 3), bold = T) %>%
  column_spec(-1, width = "6em")


```


## Important Variables

In our basic linear and LASSO model, CGPA and GRE score exert the strongest influences over a student's chance of admission. The ridge model resulted in coefficients that suggest that all variables affect the chance of admission on a similar scale; each of the variables increases the predicted chance of admission by about 1%. Conversely, the strength of a student's statement of purpose and the rating of the university have the least influence on the chance of admisison. This finding makes sense since we would expect factors that reflect a student's work and testing ability to have a greater impact on admission.

## Limitations

The ridge model, GAM and MARS model were chosen to try to maximize the predictive ability, but each of these models is limited in interpretability. Attemptingt to explain what each coefficient means would be difficult, compared to the ordinary linear regression or LASSO model. Our data contained predictors that had approximately linear relationships with chance of admission, so we believe that our models were flexible enough to capture the complexity in the data.

# Conclusion




